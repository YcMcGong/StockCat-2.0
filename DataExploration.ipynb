{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/yicong/anaconda3/envs/Standard_ML_Dev/lib/python3.5/site-packages/fix_yahoo_finance/__init__.py:43: DeprecationWarning: \n",
      "    Auto-overriding of pandas_datareader's get_data_yahoo() is deprecated and no longer available.\n",
      "    Use pdr_override() to explicitly override it.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    }
   ],
   "source": [
    "start_date = \"2013-01-01\"\n",
    "end_date = \"2017-11-02\"\n",
    "\n",
    "data_AAPL = pdr.get_data_yahoo(\"AAPL\", start=start_date, end=end_date)\n",
    "data_TSLA = pdr.get_data_yahoo(\"GOOGL\", start=start_date, end=end_date)\n",
    "data_FB = pdr.get_data_yahoo(\"FB\", start=start_date, end=end_date)\n",
    "data_NASDAQ = pdr.get_data_yahoo(\"^IXIC\", start=start_date, end=end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Open         High          Low        Close    Adj Close  \\\n",
      "Date                                                                          \n",
      "2013-01-02  3091.330078  3112.649902  3083.489990  3112.260010  3112.260010   \n",
      "2013-01-03  3108.489990  3118.179932  3092.280029  3100.570068  3100.570068   \n",
      "2013-01-04  3100.879883  3108.439941  3090.810059  3101.659912  3101.659912   \n",
      "2013-01-07  3089.169922  3102.350098  3083.879883  3098.810059  3098.810059   \n",
      "2013-01-08  3098.459961  3103.389893  3076.600098  3091.810059  3091.810059   \n",
      "2013-01-09  3099.649902  3111.219971  3096.340088  3105.810059  3105.810059   \n",
      "2013-01-10  3125.639893  3127.719971  3098.469971  3121.760010  3121.760010   \n",
      "2013-01-11  3122.120117  3126.590088  3114.100098  3125.629883  3125.629883   \n",
      "2013-01-14  3113.649902  3123.479980  3104.080078  3117.500000  3117.500000   \n",
      "2013-01-15  3101.060059  3112.290039  3093.320068  3110.780029  3110.780029   \n",
      "2013-01-16  3110.719971  3124.649902  3106.790039  3117.540039  3117.540039   \n",
      "2013-01-17  3130.489990  3144.050049  3125.790039  3136.000000  3136.000000   \n",
      "2013-01-18  3127.909912  3134.729980  3119.199951  3134.709961  3134.709961   \n",
      "2013-01-22  3135.629883  3143.179932  3121.540039  3143.179932  3143.179932   \n",
      "2013-01-23  3155.820068  3161.060059  3149.739990  3153.669922  3153.669922   \n",
      "2013-01-24  3125.669922  3153.560059  3124.449951  3130.379883  3130.379883   \n",
      "2013-01-25  3140.649902  3156.199951  3135.860107  3149.709961  3149.709961   \n",
      "2013-01-28  3152.169922  3161.830078  3144.889893  3154.300049  3154.300049   \n",
      "2013-01-29  3149.620117  3156.939941  3133.110107  3153.659912  3153.659912   \n",
      "2013-01-30  3157.429932  3164.060059  3135.830078  3142.310059  3142.310059   \n",
      "2013-01-31  3140.669922  3154.179932  3136.820068  3142.129883  3142.129883   \n",
      "2013-02-01  3162.939941  3183.139893  3154.909912  3179.100098  3179.100098   \n",
      "2013-02-04  3161.719971  3169.629883  3130.570068  3131.169922  3131.169922   \n",
      "2013-02-05  3140.899902  3178.520020  3136.820068  3171.580078  3171.580078   \n",
      "2013-02-06  3159.379883  3174.820068  3157.350098  3168.479980  3168.479980   \n",
      "2013-02-07  3167.439941  3170.419922  3135.979980  3165.129883  3165.129883   \n",
      "2013-02-08  3178.060059  3196.889893  3177.179932  3193.870117  3193.870117   \n",
      "2013-02-11  3192.530029  3194.010010  3182.189941  3192.000000  3192.000000   \n",
      "2013-02-12  3190.729980  3196.919922  3184.840088  3186.489990  3186.489990   \n",
      "2013-02-13  3195.340088  3205.520020  3187.060059  3196.879883  3196.879883   \n",
      "...                 ...          ...          ...          ...          ...   \n",
      "2017-09-22  6401.439941  6429.540039  6400.810059  6426.919922  6426.919922   \n",
      "2017-09-25  6403.109863  6408.049805  6343.959961  6370.589844  6370.589844   \n",
      "2017-09-26  6391.850098  6405.000000  6364.569824  6380.160156  6380.160156   \n",
      "2017-09-27  6414.370117  6472.649902  6405.359863  6453.259766  6453.259766   \n",
      "2017-09-28  6437.959961  6456.229980  6427.660156  6453.450195  6453.450195   \n",
      "2017-09-29  6461.279785  6497.979980  6454.859863  6495.959961  6495.959961   \n",
      "2017-10-02  6506.080078  6527.220215  6484.140137  6516.720215  6516.720215   \n",
      "2017-10-03  6523.740234  6532.180176  6509.709961  6531.709961  6531.709961   \n",
      "2017-10-04  6521.959961  6546.459961  6513.120117  6534.629883  6534.629883   \n",
      "2017-10-05  6552.870117  6587.209961  6547.649902  6585.359863  6585.359863   \n",
      "2017-10-06  6566.950195  6590.180176  6566.839844  6590.180176  6590.180176   \n",
      "2017-10-09  6597.370117  6599.339844  6572.439941  6579.729980  6579.729980   \n",
      "2017-10-10  6602.490234  6608.299805  6561.779785  6587.250000  6587.250000   \n",
      "2017-10-11  6586.729980  6604.209961  6577.990234  6603.549805  6603.549805   \n",
      "2017-10-12  6594.759766  6613.500000  6586.319824  6591.509766  6591.509766   \n",
      "2017-10-13  6613.209961  6616.580078  6602.200195  6605.799805  6605.799805   \n",
      "2017-10-16  6622.549805  6632.500000  6607.029785  6624.000000  6624.000000   \n",
      "2017-10-17  6621.419922  6628.600098  6613.209961  6623.660156  6623.660156   \n",
      "2017-10-18  6634.259766  6635.520020  6613.549805  6624.220215  6624.220215   \n",
      "2017-10-19  6583.700195  6605.290039  6558.529785  6605.069824  6605.069824   \n",
      "2017-10-20  6633.370117  6640.029785  6622.919922  6629.049805  6629.049805   \n",
      "2017-10-23  6641.569824  6641.569824  6581.149902  6586.830078  6586.830078   \n",
      "2017-10-24  6598.609863  6611.899902  6582.060059  6598.430176  6598.430176   \n",
      "2017-10-25  6587.220215  6600.640137  6517.930176  6563.890137  6563.890137   \n",
      "2017-10-26  6567.589844  6582.759766  6550.029785  6556.770020  6556.770020   \n",
      "2017-10-27  6635.029785  6708.129883  6625.779785  6701.259766  6701.259766   \n",
      "2017-10-30  6693.770020  6727.390137  6677.149902  6698.959961  6698.959961   \n",
      "2017-10-31  6713.709961  6737.750000  6705.790039  6727.669922  6727.669922   \n",
      "2017-11-01  6758.640137  6759.660156  6691.479980  6716.529785  6716.529785   \n",
      "2017-11-02  6709.390137  6719.970215  6677.549805  6714.939941  6714.939941   \n",
      "\n",
      "                Volume  \n",
      "Date                    \n",
      "2013-01-02  2111300000  \n",
      "2013-01-03  1769420000  \n",
      "2013-01-04  1745140000  \n",
      "2013-01-07  1702540000  \n",
      "2013-01-08  1744380000  \n",
      "2013-01-09  1732510000  \n",
      "2013-01-10  1754240000  \n",
      "2013-01-11  1772600000  \n",
      "2013-01-14  1876050000  \n",
      "2013-01-15  1852870000  \n",
      "2013-01-16  1692380000  \n",
      "2013-01-17  1766510000  \n",
      "2013-01-18  1860070000  \n",
      "2013-01-22  1790730000  \n",
      "2013-01-23  1698190000  \n",
      "2013-01-24  2046990000  \n",
      "2013-01-25  1920250000  \n",
      "2013-01-28  1935590000  \n",
      "2013-01-29  2050670000  \n",
      "2013-01-30  2014350000  \n",
      "2013-01-31  2190840000  \n",
      "2013-02-01  2012930000  \n",
      "2013-02-04  1874750000  \n",
      "2013-02-05  2150080000  \n",
      "2013-02-06  2002740000  \n",
      "2013-02-07  1955960000  \n",
      "2013-02-08  1816480000  \n",
      "2013-02-11  1551370000  \n",
      "2013-02-12  1786800000  \n",
      "2013-02-13  1822450000  \n",
      "...                ...  \n",
      "2017-09-22  1639100000  \n",
      "2017-09-25  2050410000  \n",
      "2017-09-26  1926420000  \n",
      "2017-09-27  2053720000  \n",
      "2017-09-28  1828570000  \n",
      "2017-09-29  1967720000  \n",
      "2017-10-02  1977030000  \n",
      "2017-10-03  1969320000  \n",
      "2017-10-04  1938440000  \n",
      "2017-10-05  1875670000  \n",
      "2017-10-06  1742470000  \n",
      "2017-10-09  1490620000  \n",
      "2017-10-10  1799400000  \n",
      "2017-10-11  1830360000  \n",
      "2017-10-12  2000470000  \n",
      "2017-10-13  1756150000  \n",
      "2017-10-16  1629480000  \n",
      "2017-10-17  1647240000  \n",
      "2017-10-18  1734030000  \n",
      "2017-10-19  1823860000  \n",
      "2017-10-20  1794810000  \n",
      "2017-10-23  1809320000  \n",
      "2017-10-24  1842640000  \n",
      "2017-10-25  2190720000  \n",
      "2017-10-26  2108210000  \n",
      "2017-10-27  2403600000  \n",
      "2017-10-30  2013640000  \n",
      "2017-10-31  2043890000  \n",
      "2017-11-01  2056860000  \n",
      "2017-11-02  2262940000  \n",
      "\n",
      "[1220 rows x 6 columns]\n",
      "1220\n"
     ]
    }
   ],
   "source": [
    "length = len(data_AAPL)\n",
    "print(data_NASDAQ)\n",
    "data_AAPL_close = (data_AAPL['Close'].values.astype('float32'))\n",
    "data_TSLA_close = (data_TSLA['Close'].values.astype('float32'))\n",
    "data_FB_close = (data_FB['Close'].values.astype('float32'))\n",
    "data_NASDAQ_close = (data_NASDAQ['Close'].values.astype('float32'))\n",
    "\n",
    "# Multiple Stock\n",
    "# data = np.vstack((data_AAPL_close, data_TSLA_close, data_FB_close, data_NASDAQ_close)).T\n",
    "# number_of_stock = len(data[0])\n",
    "# print(len(data[0]))\n",
    "\n",
    "# One Stock\n",
    "number_of_stock = 1\n",
    "data = data_FB_close\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  28.        ]\n",
      " [  27.77000046]\n",
      " [  28.76000023]\n",
      " ..., \n",
      " [ 180.05999756]\n",
      " [ 182.66000366]\n",
      " [ 178.91999817]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1220, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yahoo_stock_prices = data['Close'].values.astype('float32').reshape(length, 4)\n",
    "yahoo_stock_prices = np.reshape(data, (length,number_of_stock))\n",
    "print(yahoo_stock_prices)\n",
    "yahoo_stock_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7kopFYY0uCulVBjS4K6UUmHo/wE5MTDe7oxf8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9e87cd7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03192288]\n",
      " [ 0.03048322]\n",
      " [ 0.03668001]\n",
      " ..., \n",
      " [ 0.98372555]\n",
      " [ 1.        ]\n",
      " [ 0.9765898 ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "yahoo_stock_prices = scaler.fit_transform(yahoo_stock_prices)\n",
    "\n",
    "plt.plot(yahoo_stock_prices)\n",
    "plt.show()\n",
    "print(yahoo_stock_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 244\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(yahoo_stock_prices) * 0.80)\n",
    "test_size = len(yahoo_stock_prices) - train_size\n",
    "train, test = yahoo_stock_prices[0:train_size,:], yahoo_stock_prices[train_size:len(yahoo_stock_prices),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), :]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, :])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 30\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, train.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, number_of_stock)))\n",
    "model.add(Dense(number_of_stock))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "#Step 2 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yicong/anaconda3/envs/Standard_ML_Dev/lib/python3.5/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 897 samples, validate on 48 samples\n",
      "Epoch 1/300\n",
      "Epoch 00000: val_loss improved from inf to 0.32145, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.1151 - val_loss: 0.3214\n",
      "Epoch 2/300\n",
      "Epoch 00001: val_loss improved from 0.32145 to 0.28254, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.1005 - val_loss: 0.2825\n",
      "Epoch 3/300\n",
      "Epoch 00002: val_loss improved from 0.28254 to 0.24591, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0869 - val_loss: 0.2459\n",
      "Epoch 4/300\n",
      "Epoch 00003: val_loss improved from 0.24591 to 0.21145, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0742 - val_loss: 0.2115\n",
      "Epoch 5/300\n",
      "Epoch 00004: val_loss improved from 0.21145 to 0.17912, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0624 - val_loss: 0.1791\n",
      "Epoch 6/300\n",
      "Epoch 00005: val_loss improved from 0.17912 to 0.14884, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0516 - val_loss: 0.1488\n",
      "Epoch 7/300\n",
      "Epoch 00006: val_loss improved from 0.14884 to 0.12063, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0417 - val_loss: 0.1206\n",
      "Epoch 8/300\n",
      "Epoch 00007: val_loss improved from 0.12063 to 0.09455, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0327 - val_loss: 0.0945\n",
      "Epoch 9/300\n",
      "Epoch 00008: val_loss improved from 0.09455 to 0.07075, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0247 - val_loss: 0.0708\n",
      "Epoch 10/300\n",
      "Epoch 00009: val_loss improved from 0.07075 to 0.04954, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0178 - val_loss: 0.0495\n",
      "Epoch 11/300\n",
      "Epoch 00010: val_loss improved from 0.04954 to 0.03137, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0120 - val_loss: 0.0314\n",
      "Epoch 12/300\n",
      "Epoch 00011: val_loss improved from 0.03137 to 0.01684, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0168\n",
      "Epoch 13/300\n",
      "Epoch 00012: val_loss improved from 0.01684 to 0.00664, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0066\n",
      "Epoch 14/300\n",
      "Epoch 00013: val_loss improved from 0.00664 to 0.00128, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 15/300\n",
      "Epoch 00014: val_loss improved from 0.00128 to 0.00051, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0041 - val_loss: 5.0556e-04\n",
      "Epoch 16/300\n",
      "Epoch 00015: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0026\n",
      "Epoch 17/300\n",
      "Epoch 00016: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0087 - val_loss: 0.0048\n",
      "Epoch 18/300\n",
      "Epoch 00017: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0054\n",
      "Epoch 19/300\n",
      "Epoch 00018: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0108 - val_loss: 0.0045\n",
      "Epoch 20/300\n",
      "Epoch 00019: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0100 - val_loss: 0.0027\n",
      "Epoch 21/300\n",
      "Epoch 00020: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0012\n",
      "Epoch 22/300\n",
      "Epoch 00021: val_loss improved from 0.00051 to 0.00036, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0067 - val_loss: 3.6003e-04\n",
      "Epoch 23/300\n",
      "Epoch 00022: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0051 - val_loss: 5.7584e-04\n",
      "Epoch 24/300\n",
      "Epoch 00023: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 25/300\n",
      "Epoch 00024: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 26/300\n",
      "Epoch 00025: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0058\n",
      "Epoch 27/300\n",
      "Epoch 00026: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0081\n",
      "Epoch 28/300\n",
      "Epoch 00027: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0103\n",
      "Epoch 29/300\n",
      "Epoch 00028: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0122\n",
      "Epoch 30/300\n",
      "Epoch 00029: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0136\n",
      "Epoch 31/300\n",
      "Epoch 00030: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0145\n",
      "Epoch 32/300\n",
      "Epoch 00031: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0149\n",
      "Epoch 33/300\n",
      "Epoch 00032: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0148\n",
      "Epoch 34/300\n",
      "Epoch 00033: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0142\n",
      "Epoch 35/300\n",
      "Epoch 00034: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0037 - val_loss: 0.0133\n",
      "Epoch 36/300\n",
      "Epoch 00035: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0120\n",
      "Epoch 37/300\n",
      "Epoch 00036: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0105\n",
      "Epoch 38/300\n",
      "Epoch 00037: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0089\n",
      "Epoch 39/300\n",
      "Epoch 00038: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0073\n",
      "Epoch 40/300\n",
      "Epoch 00039: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 41/300\n",
      "Epoch 00040: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0021 - val_loss: 0.0043\n",
      "Epoch 42/300\n",
      "Epoch 00041: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 43/300\n",
      "Epoch 00042: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 44/300\n",
      "Epoch 00043: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 45/300\n",
      "Epoch 00044: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 9.2766e-04\n",
      "Epoch 46/300\n",
      "Epoch 00045: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 6.2014e-04\n",
      "Epoch 47/300\n",
      "Epoch 00046: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 4.5133e-04\n",
      "Epoch 48/300\n",
      "Epoch 00047: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 3.7141e-04\n",
      "Epoch 49/300\n",
      "Epoch 00048: val_loss improved from 0.00036 to 0.00034, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0017 - val_loss: 3.4042e-04\n",
      "Epoch 50/300\n",
      "Epoch 00049: val_loss improved from 0.00034 to 0.00033, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 0.0016 - val_loss: 3.3418e-04\n",
      "Epoch 51/300\n",
      "Epoch 00050: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0015 - val_loss: 3.4469e-04\n",
      "Epoch 52/300\n",
      "Epoch 00051: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0014 - val_loss: 3.7559e-04\n",
      "Epoch 53/300\n",
      "Epoch 00052: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0013 - val_loss: 4.3511e-04\n",
      "Epoch 54/300\n",
      "Epoch 00053: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0012 - val_loss: 5.2904e-04\n",
      "Epoch 55/300\n",
      "Epoch 00054: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0011 - val_loss: 6.5614e-04\n",
      "Epoch 56/300\n",
      "Epoch 00055: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 0.0010 - val_loss: 8.0669e-04\n",
      "Epoch 57/300\n",
      "Epoch 00056: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 9.3975e-04 - val_loss: 9.6389e-04\n",
      "Epoch 58/300\n",
      "Epoch 00057: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 8.9511e-04 - val_loss: 0.0011\n",
      "Epoch 59/300\n",
      "Epoch 00058: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 8.6561e-04 - val_loss: 0.0012\n",
      "Epoch 60/300\n",
      "Epoch 00059: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 8.4429e-04 - val_loss: 0.0013\n",
      "Epoch 61/300\n",
      "Epoch 00060: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 8.2429e-04 - val_loss: 0.0013\n",
      "Epoch 62/300\n",
      "Epoch 00061: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 8.0015e-04 - val_loss: 0.0012\n",
      "Epoch 63/300\n",
      "Epoch 00062: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 7.6861e-04 - val_loss: 0.0011\n",
      "Epoch 64/300\n",
      "Epoch 00063: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 7.2891e-04 - val_loss: 9.5760e-04\n",
      "Epoch 65/300\n",
      "Epoch 00064: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 6.8259e-04 - val_loss: 7.9158e-04\n",
      "Epoch 66/300\n",
      "Epoch 00065: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 6.3288e-04 - val_loss: 6.2899e-04\n",
      "Epoch 67/300\n",
      "Epoch 00066: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 5.8385e-04 - val_loss: 4.8912e-04\n",
      "Epoch 68/300\n",
      "Epoch 00067: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 5.3943e-04 - val_loss: 3.8630e-04\n",
      "Epoch 69/300\n",
      "Epoch 00068: val_loss improved from 0.00033 to 0.00033, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 5.0255e-04 - val_loss: 3.2766e-04\n",
      "Epoch 70/300\n",
      "Epoch 00069: val_loss improved from 0.00033 to 0.00031, saving model to saved_models/weights.test_run.hdf5\n",
      "897/897 [==============================] - 0s - loss: 4.7452e-04 - val_loss: 3.1223e-04\n",
      "Epoch 71/300\n",
      "Epoch 00070: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 4.5482e-04 - val_loss: 3.3173e-04\n",
      "Epoch 72/300\n",
      "Epoch 00071: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 4.4133e-04 - val_loss: 3.7290e-04\n",
      "Epoch 73/300\n",
      "Epoch 00072: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 4.3107e-04 - val_loss: 4.2083e-04\n",
      "Epoch 74/300\n",
      "Epoch 00073: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 4.2100e-04 - val_loss: 4.6246e-04\n",
      "Epoch 75/300\n",
      "Epoch 00074: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 4.0894e-04 - val_loss: 4.8913e-04\n",
      "Epoch 76/300\n",
      "Epoch 00075: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.9405e-04 - val_loss: 4.9770e-04\n",
      "Epoch 77/300\n",
      "Epoch 00076: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.7689e-04 - val_loss: 4.8995e-04\n",
      "Epoch 78/300\n",
      "Epoch 00077: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.5906e-04 - val_loss: 4.7092e-04\n",
      "Epoch 79/300\n",
      "Epoch 00078: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.4251e-04 - val_loss: 4.4674e-04\n",
      "Epoch 80/300\n",
      "Epoch 00079: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.2887e-04 - val_loss: 4.2284e-04\n",
      "Epoch 81/300\n",
      "Epoch 00080: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.1901e-04 - val_loss: 4.0298e-04\n",
      "Epoch 82/300\n",
      "Epoch 00081: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.1284e-04 - val_loss: 3.8912e-04\n",
      "Epoch 83/300\n",
      "Epoch 00082: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.0950e-04 - val_loss: 3.8199e-04\n",
      "Epoch 84/300\n",
      "Epoch 00083: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.0770e-04 - val_loss: 3.8184e-04\n",
      "Epoch 85/300\n",
      "Epoch 00084: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.0617e-04 - val_loss: 3.8913e-04\n",
      "Epoch 86/300\n",
      "Epoch 00085: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.0402e-04 - val_loss: 4.0479e-04\n",
      "Epoch 87/300\n",
      "Epoch 00086: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 3.0089e-04 - val_loss: 4.3001e-04\n",
      "Epoch 88/300\n",
      "Epoch 00087: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.9697e-04 - val_loss: 4.6571e-04\n",
      "Epoch 89/300\n",
      "Epoch 00088: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.9286e-04 - val_loss: 5.1179e-04\n",
      "Epoch 90/300\n",
      "Epoch 00089: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8925e-04 - val_loss: 5.6657e-04\n",
      "Epoch 91/300\n",
      "Epoch 00090: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8672e-04 - val_loss: 6.2657e-04\n",
      "Epoch 92/300\n",
      "Epoch 00091: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8550e-04 - val_loss: 6.8682e-04\n",
      "Epoch 93/300\n",
      "Epoch 00092: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8546e-04 - val_loss: 7.4167e-04\n",
      "Epoch 94/300\n",
      "Epoch 00093: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8615e-04 - val_loss: 7.8585e-04\n",
      "Epoch 95/300\n",
      "Epoch 00094: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8702e-04 - val_loss: 8.1550e-04\n",
      "Epoch 96/300\n",
      "Epoch 00095: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8759e-04 - val_loss: 8.2891e-04\n",
      "Epoch 97/300\n",
      "Epoch 00096: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8760e-04 - val_loss: 8.2670e-04\n",
      "Epoch 98/300\n",
      "Epoch 00097: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8708e-04 - val_loss: 8.1152e-04\n",
      "Epoch 99/300\n",
      "Epoch 00098: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8623e-04 - val_loss: 7.8732e-04\n",
      "Epoch 100/300\n",
      "Epoch 00099: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8539e-04 - val_loss: 7.5851e-04\n",
      "Epoch 101/300\n",
      "Epoch 00100: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8480e-04 - val_loss: 7.2906e-04\n",
      "Epoch 102/300\n",
      "Epoch 00101: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8459e-04 - val_loss: 7.0262e-04\n",
      "Epoch 103/300\n",
      "Epoch 00102: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8473e-04 - val_loss: 6.8140e-04\n",
      "Epoch 104/300\n",
      "Epoch 00103: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8505e-04 - val_loss: 6.6678e-04\n",
      "Epoch 105/300\n",
      "Epoch 00104: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8535e-04 - val_loss: 6.5929e-04\n",
      "Epoch 106/300\n",
      "Epoch 00105: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8545e-04 - val_loss: 6.5880e-04\n",
      "Epoch 107/300\n",
      "Epoch 00106: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8528e-04 - val_loss: 6.6462e-04\n",
      "Epoch 108/300\n",
      "Epoch 00107: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8486e-04 - val_loss: 6.7559e-04\n",
      "Epoch 109/300\n",
      "Epoch 00108: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8429e-04 - val_loss: 6.9013e-04\n",
      "Epoch 110/300\n",
      "Epoch 00109: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8371e-04 - val_loss: 7.0635e-04\n",
      "Epoch 111/300\n",
      "Epoch 00110: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 0s - loss: 2.8322e-04 - val_loss: 7.2217e-04\n",
      "Epoch 112/300\n",
      "Epoch 00111: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8285e-04 - val_loss: 7.3555e-04\n",
      "Epoch 113/300\n",
      "Epoch 00112: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8261e-04 - val_loss: 7.4476e-04\n",
      "Epoch 114/300\n",
      "Epoch 00113: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8242e-04 - val_loss: 7.4861e-04\n",
      "Epoch 115/300\n",
      "Epoch 00114: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8222e-04 - val_loss: 7.4658e-04\n",
      "Epoch 116/300\n",
      "Epoch 00115: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8193e-04 - val_loss: 7.3890e-04\n",
      "Epoch 117/300\n",
      "Epoch 00116: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8155e-04 - val_loss: 7.2646e-04\n",
      "Epoch 118/300\n",
      "Epoch 00117: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8110e-04 - val_loss: 7.1058e-04\n",
      "Epoch 119/300\n",
      "Epoch 00118: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8062e-04 - val_loss: 6.9283e-04\n",
      "Epoch 120/300\n",
      "Epoch 00119: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.8018e-04 - val_loss: 6.7477e-04\n",
      "Epoch 121/300\n",
      "Epoch 00120: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7979e-04 - val_loss: 6.5777e-04\n",
      "Epoch 122/300\n",
      "Epoch 00121: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7946e-04 - val_loss: 6.4286e-04\n",
      "Epoch 123/300\n",
      "Epoch 00122: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7919e-04 - val_loss: 6.3073e-04\n",
      "Epoch 124/300\n",
      "Epoch 00123: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7895e-04 - val_loss: 6.2171e-04\n",
      "Epoch 125/300\n",
      "Epoch 00124: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7869e-04 - val_loss: 6.1579e-04\n",
      "Epoch 126/300\n",
      "Epoch 00125: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7841e-04 - val_loss: 6.1269e-04\n",
      "Epoch 127/300\n",
      "Epoch 00126: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7812e-04 - val_loss: 6.1192e-04\n",
      "Epoch 128/300\n",
      "Epoch 00127: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7781e-04 - val_loss: 6.1280e-04\n",
      "Epoch 129/300\n",
      "Epoch 00128: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7750e-04 - val_loss: 6.1461e-04\n",
      "Epoch 130/300\n",
      "Epoch 00129: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7723e-04 - val_loss: 6.1656e-04\n",
      "Epoch 131/300\n",
      "Epoch 00130: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7698e-04 - val_loss: 6.1796e-04\n",
      "Epoch 132/300\n",
      "Epoch 00131: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7676e-04 - val_loss: 6.1823e-04\n",
      "Epoch 133/300\n",
      "Epoch 00132: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7656e-04 - val_loss: 6.1699e-04\n",
      "Epoch 134/300\n",
      "Epoch 00133: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7636e-04 - val_loss: 6.1411e-04\n",
      "Epoch 135/300\n",
      "Epoch 00134: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7615e-04 - val_loss: 6.0965e-04\n",
      "Epoch 136/300\n",
      "Epoch 00135: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7594e-04 - val_loss: 6.0389e-04\n",
      "Epoch 137/300\n",
      "Epoch 00136: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7572e-04 - val_loss: 5.9723e-04\n",
      "Epoch 138/300\n",
      "Epoch 00137: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7550e-04 - val_loss: 5.9017e-04\n",
      "Epoch 139/300\n",
      "Epoch 00138: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7528e-04 - val_loss: 5.8320e-04\n",
      "Epoch 140/300\n",
      "Epoch 00139: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7508e-04 - val_loss: 5.7676e-04\n",
      "Epoch 141/300\n",
      "Epoch 00140: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7488e-04 - val_loss: 5.7121e-04\n",
      "Epoch 142/300\n",
      "Epoch 00141: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7470e-04 - val_loss: 5.6676e-04\n",
      "Epoch 143/300\n",
      "Epoch 00142: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7451e-04 - val_loss: 5.6351e-04\n",
      "Epoch 144/300\n",
      "Epoch 00143: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7432e-04 - val_loss: 5.6145e-04\n",
      "Epoch 145/300\n",
      "Epoch 00144: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7413e-04 - val_loss: 5.6043e-04\n",
      "Epoch 146/300\n",
      "Epoch 00145: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7393e-04 - val_loss: 5.6025e-04\n",
      "Epoch 147/300\n",
      "Epoch 00146: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7373e-04 - val_loss: 5.6062e-04\n",
      "Epoch 148/300\n",
      "Epoch 00147: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7353e-04 - val_loss: 5.6124e-04\n",
      "Epoch 149/300\n",
      "Epoch 00148: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7334e-04 - val_loss: 5.6183e-04\n",
      "Epoch 150/300\n",
      "Epoch 00149: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7314e-04 - val_loss: 5.6212e-04\n",
      "Epoch 151/300\n",
      "Epoch 00150: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7296e-04 - val_loss: 5.6194e-04\n",
      "Epoch 152/300\n",
      "Epoch 00151: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7277e-04 - val_loss: 5.6117e-04\n",
      "Epoch 153/300\n",
      "Epoch 00152: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7258e-04 - val_loss: 5.5980e-04\n",
      "Epoch 154/300\n",
      "Epoch 00153: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7239e-04 - val_loss: 5.5789e-04\n",
      "Epoch 155/300\n",
      "Epoch 00154: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7219e-04 - val_loss: 5.5556e-04\n",
      "Epoch 156/300\n",
      "Epoch 00155: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7200e-04 - val_loss: 5.5297e-04\n",
      "Epoch 157/300\n",
      "Epoch 00156: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7181e-04 - val_loss: 5.5033e-04\n",
      "Epoch 158/300\n",
      "Epoch 00157: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7162e-04 - val_loss: 5.4780e-04\n",
      "Epoch 159/300\n",
      "Epoch 00158: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7143e-04 - val_loss: 5.4553e-04\n",
      "Epoch 160/300\n",
      "Epoch 00159: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7124e-04 - val_loss: 5.4363e-04\n",
      "Epoch 161/300\n",
      "Epoch 00160: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7105e-04 - val_loss: 5.4216e-04\n",
      "Epoch 162/300\n",
      "Epoch 00161: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7087e-04 - val_loss: 5.4111e-04\n",
      "Epoch 163/300\n",
      "Epoch 00162: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7068e-04 - val_loss: 5.4045e-04\n",
      "Epoch 164/300\n",
      "Epoch 00163: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7049e-04 - val_loss: 5.4008e-04\n",
      "Epoch 165/300\n",
      "Epoch 00164: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7031e-04 - val_loss: 5.3990e-04\n",
      "Epoch 166/300\n",
      "Epoch 00165: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.7012e-04 - val_loss: 5.3978e-04\n",
      "Epoch 167/300\n",
      "Epoch 00166: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6994e-04 - val_loss: 5.3960e-04\n",
      "Epoch 168/300\n",
      "Epoch 00167: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6976e-04 - val_loss: 5.3927e-04\n",
      "Epoch 169/300\n",
      "Epoch 00168: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6958e-04 - val_loss: 5.3870e-04\n",
      "Epoch 170/300\n",
      "Epoch 00169: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6940e-04 - val_loss: 5.3786e-04\n",
      "Epoch 171/300\n",
      "Epoch 00170: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6922e-04 - val_loss: 5.3674e-04\n",
      "Epoch 172/300\n",
      "Epoch 00171: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6904e-04 - val_loss: 5.3538e-04\n",
      "Epoch 173/300\n",
      "Epoch 00172: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6886e-04 - val_loss: 5.3383e-04\n",
      "Epoch 174/300\n",
      "Epoch 00173: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6868e-04 - val_loss: 5.3216e-04\n",
      "Epoch 175/300\n",
      "Epoch 00174: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6850e-04 - val_loss: 5.3045e-04\n",
      "Epoch 176/300\n",
      "Epoch 00175: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6833e-04 - val_loss: 5.2878e-04\n",
      "Epoch 177/300\n",
      "Epoch 00176: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6815e-04 - val_loss: 5.2720e-04\n",
      "Epoch 178/300\n",
      "Epoch 00177: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6798e-04 - val_loss: 5.2575e-04\n",
      "Epoch 179/300\n",
      "Epoch 00178: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6780e-04 - val_loss: 5.2446e-04\n",
      "Epoch 180/300\n",
      "Epoch 00179: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6763e-04 - val_loss: 5.2332e-04\n",
      "Epoch 181/300\n",
      "Epoch 00180: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6746e-04 - val_loss: 5.2230e-04\n",
      "Epoch 182/300\n",
      "Epoch 00181: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6728e-04 - val_loss: 5.2138e-04\n",
      "Epoch 183/300\n",
      "Epoch 00182: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6711e-04 - val_loss: 5.2049e-04\n",
      "Epoch 184/300\n",
      "Epoch 00183: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6694e-04 - val_loss: 5.1959e-04\n",
      "Epoch 185/300\n",
      "Epoch 00184: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6677e-04 - val_loss: 5.1864e-04\n",
      "Epoch 186/300\n",
      "Epoch 00185: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6660e-04 - val_loss: 5.1761e-04\n",
      "Epoch 187/300\n",
      "Epoch 00186: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6644e-04 - val_loss: 5.1646e-04\n",
      "Epoch 188/300\n",
      "Epoch 00187: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6627e-04 - val_loss: 5.1520e-04\n",
      "Epoch 189/300\n",
      "Epoch 00188: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6610e-04 - val_loss: 5.1384e-04\n",
      "Epoch 190/300\n",
      "Epoch 00189: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6594e-04 - val_loss: 5.1239e-04\n",
      "Epoch 191/300\n",
      "Epoch 00190: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6577e-04 - val_loss: 5.1088e-04\n",
      "Epoch 192/300\n",
      "Epoch 00191: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6561e-04 - val_loss: 5.0935e-04\n",
      "Epoch 193/300\n",
      "Epoch 00192: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6544e-04 - val_loss: 5.0783e-04\n",
      "Epoch 194/300\n",
      "Epoch 00193: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6528e-04 - val_loss: 5.0634e-04\n",
      "Epoch 195/300\n",
      "Epoch 00194: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6512e-04 - val_loss: 5.0492e-04\n",
      "Epoch 196/300\n",
      "Epoch 00195: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6495e-04 - val_loss: 5.0356e-04\n",
      "Epoch 197/300\n",
      "Epoch 00196: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6479e-04 - val_loss: 5.0227e-04\n",
      "Epoch 198/300\n",
      "Epoch 00197: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6463e-04 - val_loss: 5.0104e-04\n",
      "Epoch 199/300\n",
      "Epoch 00198: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6447e-04 - val_loss: 4.9985e-04\n",
      "Epoch 200/300\n",
      "Epoch 00199: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6431e-04 - val_loss: 4.9870e-04\n",
      "Epoch 201/300\n",
      "Epoch 00200: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6415e-04 - val_loss: 4.9755e-04\n",
      "Epoch 202/300\n",
      "Epoch 00201: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6400e-04 - val_loss: 4.9640e-04\n",
      "Epoch 203/300\n",
      "Epoch 00202: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6384e-04 - val_loss: 4.9522e-04\n",
      "Epoch 204/300\n",
      "Epoch 00203: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6368e-04 - val_loss: 4.9400e-04\n",
      "Epoch 205/300\n",
      "Epoch 00204: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6353e-04 - val_loss: 4.9275e-04\n",
      "Epoch 206/300\n",
      "Epoch 00205: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6337e-04 - val_loss: 4.9146e-04\n",
      "Epoch 207/300\n",
      "Epoch 00206: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6322e-04 - val_loss: 4.9016e-04\n",
      "Epoch 208/300\n",
      "Epoch 00207: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6307e-04 - val_loss: 4.8883e-04\n",
      "Epoch 209/300\n",
      "Epoch 00208: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6291e-04 - val_loss: 4.8751e-04\n",
      "Epoch 210/300\n",
      "Epoch 00209: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6276e-04 - val_loss: 4.8620e-04\n",
      "Epoch 211/300\n",
      "Epoch 00210: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6261e-04 - val_loss: 4.8491e-04\n",
      "Epoch 212/300\n",
      "Epoch 00211: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6246e-04 - val_loss: 4.8366e-04\n",
      "Epoch 213/300\n",
      "Epoch 00212: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6231e-04 - val_loss: 4.8244e-04\n",
      "Epoch 214/300\n",
      "Epoch 00213: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6216e-04 - val_loss: 4.8126e-04\n",
      "Epoch 215/300\n",
      "Epoch 00214: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6201e-04 - val_loss: 4.8011e-04\n",
      "Epoch 216/300\n",
      "Epoch 00215: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6186e-04 - val_loss: 4.7899e-04\n",
      "Epoch 217/300\n",
      "Epoch 00216: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6171e-04 - val_loss: 4.7788e-04\n",
      "Epoch 218/300\n",
      "Epoch 00217: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6157e-04 - val_loss: 4.7678e-04\n",
      "Epoch 219/300\n",
      "Epoch 00218: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6142e-04 - val_loss: 4.7568e-04\n",
      "Epoch 220/300\n",
      "Epoch 00219: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6128e-04 - val_loss: 4.7457e-04\n",
      "Epoch 221/300\n",
      "Epoch 00220: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6113e-04 - val_loss: 4.7346e-04\n",
      "Epoch 222/300\n",
      "Epoch 00221: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6099e-04 - val_loss: 4.7233e-04\n",
      "Epoch 223/300\n",
      "Epoch 00222: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6085e-04 - val_loss: 4.7120e-04\n",
      "Epoch 224/300\n",
      "Epoch 00223: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6070e-04 - val_loss: 4.7007e-04\n",
      "Epoch 225/300\n",
      "Epoch 00224: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6056e-04 - val_loss: 4.6893e-04\n",
      "Epoch 226/300\n",
      "Epoch 00225: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6042e-04 - val_loss: 4.6780e-04\n",
      "Epoch 227/300\n",
      "Epoch 00226: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 0s - loss: 2.6028e-04 - val_loss: 4.6668e-04\n",
      "Epoch 228/300\n",
      "Epoch 00227: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6014e-04 - val_loss: 4.6558e-04\n",
      "Epoch 229/300\n",
      "Epoch 00228: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.6000e-04 - val_loss: 4.6450e-04\n",
      "Epoch 230/300\n",
      "Epoch 00229: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5986e-04 - val_loss: 4.6343e-04\n",
      "Epoch 231/300\n",
      "Epoch 00230: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5973e-04 - val_loss: 4.6238e-04\n",
      "Epoch 232/300\n",
      "Epoch 00231: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5959e-04 - val_loss: 4.6134e-04\n",
      "Epoch 233/300\n",
      "Epoch 00232: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5945e-04 - val_loss: 4.6032e-04\n",
      "Epoch 234/300\n",
      "Epoch 00233: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5932e-04 - val_loss: 4.5931e-04\n",
      "Epoch 235/300\n",
      "Epoch 00234: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5918e-04 - val_loss: 4.5830e-04\n",
      "Epoch 236/300\n",
      "Epoch 00235: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5905e-04 - val_loss: 4.5728e-04\n",
      "Epoch 237/300\n",
      "Epoch 00236: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5891e-04 - val_loss: 4.5627e-04\n",
      "Epoch 238/300\n",
      "Epoch 00237: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5878e-04 - val_loss: 4.5526e-04\n",
      "Epoch 239/300\n",
      "Epoch 00238: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5865e-04 - val_loss: 4.5425e-04\n",
      "Epoch 240/300\n",
      "Epoch 00239: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5852e-04 - val_loss: 4.5323e-04\n",
      "Epoch 241/300\n",
      "Epoch 00240: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5838e-04 - val_loss: 4.5222e-04\n",
      "Epoch 242/300\n",
      "Epoch 00241: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5825e-04 - val_loss: 4.5121e-04\n",
      "Epoch 243/300\n",
      "Epoch 00242: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5812e-04 - val_loss: 4.5021e-04\n",
      "Epoch 244/300\n",
      "Epoch 00243: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5799e-04 - val_loss: 4.4922e-04\n",
      "Epoch 245/300\n",
      "Epoch 00244: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5787e-04 - val_loss: 4.4823e-04\n",
      "Epoch 246/300\n",
      "Epoch 00245: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5774e-04 - val_loss: 4.4726e-04\n",
      "Epoch 247/300\n",
      "Epoch 00246: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5761e-04 - val_loss: 4.4630e-04\n",
      "Epoch 248/300\n",
      "Epoch 00247: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5748e-04 - val_loss: 4.4534e-04\n",
      "Epoch 249/300\n",
      "Epoch 00248: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5736e-04 - val_loss: 4.4439e-04\n",
      "Epoch 250/300\n",
      "Epoch 00249: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5723e-04 - val_loss: 4.4345e-04\n",
      "Epoch 251/300\n",
      "Epoch 00250: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5711e-04 - val_loss: 4.4251e-04\n",
      "Epoch 252/300\n",
      "Epoch 00251: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5698e-04 - val_loss: 4.4158e-04\n",
      "Epoch 253/300\n",
      "Epoch 00252: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5686e-04 - val_loss: 4.4065e-04\n",
      "Epoch 254/300\n",
      "Epoch 00253: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5674e-04 - val_loss: 4.3972e-04\n",
      "Epoch 255/300\n",
      "Epoch 00254: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5661e-04 - val_loss: 4.3879e-04\n",
      "Epoch 256/300\n",
      "Epoch 00255: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5649e-04 - val_loss: 4.3786e-04\n",
      "Epoch 257/300\n",
      "Epoch 00256: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5637e-04 - val_loss: 4.3694e-04\n",
      "Epoch 258/300\n",
      "Epoch 00257: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5625e-04 - val_loss: 4.3602e-04\n",
      "Epoch 259/300\n",
      "Epoch 00258: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5613e-04 - val_loss: 4.3511e-04\n",
      "Epoch 260/300\n",
      "Epoch 00259: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5601e-04 - val_loss: 4.3421e-04\n",
      "Epoch 261/300\n",
      "Epoch 00260: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5589e-04 - val_loss: 4.3331e-04\n",
      "Epoch 262/300\n",
      "Epoch 00261: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5577e-04 - val_loss: 4.3242e-04\n",
      "Epoch 263/300\n",
      "Epoch 00262: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5565e-04 - val_loss: 4.3153e-04\n",
      "Epoch 264/300\n",
      "Epoch 00263: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5554e-04 - val_loss: 4.3065e-04\n",
      "Epoch 265/300\n",
      "Epoch 00264: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5542e-04 - val_loss: 4.2978e-04\n",
      "Epoch 266/300\n",
      "Epoch 00265: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5530e-04 - val_loss: 4.2891e-04\n",
      "Epoch 267/300\n",
      "Epoch 00266: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5519e-04 - val_loss: 4.2805e-04\n",
      "Epoch 268/300\n",
      "Epoch 00267: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5507e-04 - val_loss: 4.2720e-04\n",
      "Epoch 269/300\n",
      "Epoch 00268: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5496e-04 - val_loss: 4.2634e-04\n",
      "Epoch 270/300\n",
      "Epoch 00269: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5485e-04 - val_loss: 4.2549e-04\n",
      "Epoch 271/300\n",
      "Epoch 00270: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5473e-04 - val_loss: 4.2465e-04\n",
      "Epoch 272/300\n",
      "Epoch 00271: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5462e-04 - val_loss: 4.2381e-04\n",
      "Epoch 273/300\n",
      "Epoch 00272: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5451e-04 - val_loss: 4.2297e-04\n",
      "Epoch 274/300\n",
      "Epoch 00273: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5440e-04 - val_loss: 4.2214e-04\n",
      "Epoch 275/300\n",
      "Epoch 00274: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5429e-04 - val_loss: 4.2131e-04\n",
      "Epoch 276/300\n",
      "Epoch 00275: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5418e-04 - val_loss: 4.2049e-04\n",
      "Epoch 277/300\n",
      "Epoch 00276: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5407e-04 - val_loss: 4.1967e-04\n",
      "Epoch 278/300\n",
      "Epoch 00277: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5396e-04 - val_loss: 4.1886e-04\n",
      "Epoch 279/300\n",
      "Epoch 00278: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5385e-04 - val_loss: 4.1806e-04\n",
      "Epoch 280/300\n",
      "Epoch 00279: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5374e-04 - val_loss: 4.1726e-04\n",
      "Epoch 281/300\n",
      "Epoch 00280: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5363e-04 - val_loss: 4.1647e-04\n",
      "Epoch 282/300\n",
      "Epoch 00281: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5352e-04 - val_loss: 4.1568e-04\n",
      "Epoch 283/300\n",
      "Epoch 00282: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5342e-04 - val_loss: 4.1489e-04\n",
      "Epoch 284/300\n",
      "Epoch 00283: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5331e-04 - val_loss: 4.1411e-04\n",
      "Epoch 285/300\n",
      "Epoch 00284: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5320e-04 - val_loss: 4.1334e-04\n",
      "Epoch 286/300\n",
      "Epoch 00285: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5310e-04 - val_loss: 4.1257e-04\n",
      "Epoch 287/300\n",
      "Epoch 00286: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5299e-04 - val_loss: 4.1181e-04\n",
      "Epoch 288/300\n",
      "Epoch 00287: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5289e-04 - val_loss: 4.1104e-04\n",
      "Epoch 289/300\n",
      "Epoch 00288: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5279e-04 - val_loss: 4.1029e-04\n",
      "Epoch 290/300\n",
      "Epoch 00289: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5268e-04 - val_loss: 4.0954e-04\n",
      "Epoch 291/300\n",
      "Epoch 00290: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5258e-04 - val_loss: 4.0879e-04\n",
      "Epoch 292/300\n",
      "Epoch 00291: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5248e-04 - val_loss: 4.0805e-04\n",
      "Epoch 293/300\n",
      "Epoch 00292: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5238e-04 - val_loss: 4.0731e-04\n",
      "Epoch 294/300\n",
      "Epoch 00293: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5228e-04 - val_loss: 4.0658e-04\n",
      "Epoch 295/300\n",
      "Epoch 00294: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5217e-04 - val_loss: 4.0585e-04\n",
      "Epoch 296/300\n",
      "Epoch 00295: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5207e-04 - val_loss: 4.0513e-04\n",
      "Epoch 297/300\n",
      "Epoch 00296: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5197e-04 - val_loss: 4.0441e-04\n",
      "Epoch 298/300\n",
      "Epoch 00297: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5187e-04 - val_loss: 4.0370e-04\n",
      "Epoch 299/300\n",
      "Epoch 00298: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5178e-04 - val_loss: 4.0299e-04\n",
      "Epoch 300/300\n",
      "Epoch 00299: val_loss did not improve\n",
      "897/897 [==============================] - 0s - loss: 2.5168e-04 - val_loss: 4.0228e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe97ce44da0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "epochs = 300\n",
    "batch_size = 1000\n",
    "\n",
    "# model.load_weights('saved_models/weights.test_run')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.test_run.hdf5', \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(trainX, trainY, nb_epoch=epochs, batch_size=batch_size, validation_split=0.05, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "model.load_weights('saved_models/weights.test_run.hdf5')\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# invert predictions and targets to unscaled\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 5.84 RMSE\n",
      "Test Score: 1.80 RMSE\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shift predictions of training data for plotting\n",
    "trainPredictPlot = np.empty_like(yahoo_stock_prices)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift predictions of test data for plotting\n",
    "testPredictPlot = np.empty_like(yahoo_stock_prices)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(yahoo_stock_prices)-1, :] = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkooz6CWpuDssPio+m3yf6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe97c695ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot baseline and predictions\n",
    "ground_truth = scaler.inverse_transform(yahoo_stock_prices)\n",
    "# plt.plot(scaler.inverse_transform(yahoo_stock_prices))\n",
    "# print(yahoo_stock_prices.shape)\n",
    "plt.plot(ground_truth[:,0])\n",
    "plt.plot(trainPredictPlot[:,0], 'orange')\n",
    "plt.plot(testPredictPlot[:,0],'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predict = model.predict(yahoo_stock_prices)\n",
    "# length_total = len(yahoo_stock_prices)\n",
    "# day = 93\n",
    "# test = yahoo_stock_prices[length_total-30-day:length_total-day].reshape(1,30,1)\n",
    "# Predict = model.predict(test)\n",
    "# print(scaler.inverse_transform(Predict))\n",
    "# print(scaler.inverse_transform(yahoo_stock_prices[length_total-day].reshape(-1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
